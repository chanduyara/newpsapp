{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = \"ner_dataset.csv\"  # Ensure the dataset is in the same directory\n",
    "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #       Word  POS Tag\n",
       "0  Sentence: 1  Thousands  NNS   O\n",
       "1          NaN         of   IN   O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing sentence numbers\n",
    "df[\"Sentence #\"] = df[\"Sentence #\"].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #       Word  POS Tag\n",
       "0  Sentence: 1  Thousands  NNS   O\n",
       "1  Sentence: 1         of   IN   O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop POS column\n",
    "df = df.drop(columns=[\"POS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group words by sentence\n",
    "sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "tags = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([list(['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.']),\n",
       "        list(['Iranian', 'officials', 'say', 'they', 'expect', 'to', 'get', 'access', 'to', 'sealed', 'sensitive', 'parts', 'of', 'the', 'plant', 'Wednesday', ',', 'after', 'an', 'IAEA', 'surveillance', 'system', 'begins', 'functioning', '.']),\n",
       "        list(['Helicopter', 'gunships', 'Saturday', 'pounded', 'militant', 'hideouts', 'in', 'the', 'Orakzai', 'tribal', 'region', ',', 'where', 'many', 'Taliban', 'militants', 'are', 'believed', 'to', 'have', 'fled', 'to', 'avoid', 'an', 'earlier', 'military', 'offensive', 'in', 'nearby', 'South', 'Waziristan', '.']),\n",
       "        ...,\n",
       "        list(['Following', 'Iran', \"'s\", 'disputed', 'June', '12', 'elections', ',', 'rights', 'groups', 'said', 'hundreds', 'of', 'people', 'were', 'detained', 'in', 'clashes', 'with', 'security', 'forces', 'during', 'post-election', ',', 'anti-government', 'demonstrations', '.']),\n",
       "        list(['Since', 'then', ',', 'authorities', 'have', 'held', 'public', 'trials', 'of', 'the', 'accused', 'and', 'tried', 'to', 'marginalize', 'moderate', 'officials', 'within', 'the', 'government', '.']),\n",
       "        list(['The', 'United', 'Nations', 'is', 'praising', 'the', 'use', 'of', 'military', 'helicopters', 'to', 'drop', 'food', 'and', 'rescue', 'survivors', 'in', 'tsunami-ravaged', 'Indonesia', ',', 'saying', 'the', 'aircraft', 'are', '\"', 'worth', 'their', 'weight', 'in', 'gold', '.', '\"'])],\n",
       "       dtype=object),\n",
       " array([list(['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']),\n",
       "        list(['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O']),\n",
       "        list(['O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O']),\n",
       "        ...,\n",
       "        list(['O', 'B-geo', 'O', 'O', 'B-tim', 'I-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']),\n",
       "        list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']),\n",
       "        list(['O', 'B-org', 'I-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'I-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences,tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset (70% train, 10% validation, 20% test)\n",
    "train_sentences, test_sentences, train_tags, test_tags = train_test_split(sentences, tags, test_size=0.2, random_state=42)\n",
    "train_sentences, val_sentences, train_tags, val_tags = train_test_split(train_sentences, train_tags, test_size=0.125, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33571,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words and labels\n",
    "word_tokenizer = Tokenizer(lower=False, oov_token=\"<OOV>\")\n",
    "word_tokenizer.fit_on_texts(train_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_tokenizer = Tokenizer(lower=False)\n",
    "tag_tokenizer.fit_on_texts(train_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Medina', 'Garrigues', 'raced', 'to', 'a', '05-Jan', 'lead', 'in', 'the', 'first', 'set', 'before', 'Cho', 'won', 'the', 'next', 'three', 'games', '.']),\n",
       "       list(['A', 'similar', 'oil', 'loan', 'program', 'aided', 'oil', 'companies', 'after', 'a', 'hurricane', 'disrupted', 'production', 'last', 'year', '.']),\n",
       "       list(['The', 'United', 'Nations', 'says', 'it', 'is', 'rushing', 'assistance', 'to', 'El', 'Salvador', 'and', 'Costa', 'Rica', ',', 'and', 'remains', 'ready', 'to', 'mobilize', 'international', 'support', 'for', 'emergency', 'relief', 'and', 'recovery', 'efforts', '.']),\n",
       "       ...,\n",
       "       list(['The', 'Trees', 'were', 'good-natured', 'and', 'gave', 'him', 'one', 'of', 'their', 'branches', '.']),\n",
       "       list(['He', 'said', 'Hutu', 'rebels', 'in', 'Congo', 'are', 'massing', 'near', 'the', 'border', 'with', 'Rwanda', ',', 'adding', 'that', 'the', 'government', 'will', 'take', 'any', 'means', 'necessary', 'to', 'defend', 'Rwandan', 'territory', '.']),\n",
       "       list(['Dr.', 'Lee', 'says', 'poultry', 'products', 'are', 'an', 'important', 'source', 'of', 'protein', 'and', 'that', 'thorough', 'cooking', 'kills', 'the', 'virus', '.'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words and tags to sequences\n",
    "X_train = word_tokenizer.texts_to_sequences(train_sentences)\n",
    "X_val = word_tokenizer.texts_to_sequences(val_sentences)\n",
    "X_test = word_tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "y_train = tag_tokenizer.texts_to_sequences(train_tags)\n",
    "y_val = tag_tokenizer.texts_to_sequences(val_tags)\n",
    "y_test = tag_tokenizer.texts_to_sequences(test_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences to max sentence length\n",
    "max_len = max(len(seq) for seq in X_train)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding=\"post\")\n",
    "X_val = pad_sequences(X_val, maxlen=max_len, padding=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "y_train = pad_sequences(y_train, maxlen=max_len, padding=\"post\")\n",
    "y_val = pad_sequences(y_val, maxlen=max_len, padding=\"post\")\n",
    "y_test = pad_sequences(y_test, maxlen=max_len, padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical format\n",
    "num_tags = len(tag_tokenizer.word_index) + 1  # Add 1 for padding index\n",
    "y_train = to_categorical(y_train, num_classes=num_tags)\n",
    "y_val = to_categorical(y_val, num_classes=num_tags)\n",
    "y_test = to_categorical(y_test, num_classes=num_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5344,  6617, 10418, ...,     0,     0,     0],\n",
       "       [   48,   919,   106, ...,     0,     0,     0],\n",
       "       [   10,    51,   202, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [   10, 15355,    32, ...,     0,     0,     0],\n",
       "       [   62,    18,  3052, ...,     0,     0,     0],\n",
       "       [ 3646,  3278,    21, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - accuracy: 0.9200 - loss: 0.3149 - val_accuracy: 0.9727 - val_loss: 0.1023\n",
      "Epoch 2/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - accuracy: 0.9742 - loss: 0.0915 - val_accuracy: 0.9778 - val_loss: 0.0763\n",
      "Epoch 3/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.9796 - loss: 0.0676 - val_accuracy: 0.9804 - val_loss: 0.0683\n",
      "Epoch 4/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 20ms/step - accuracy: 0.9829 - loss: 0.0564 - val_accuracy: 0.9824 - val_loss: 0.0634\n",
      "Epoch 5/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9859 - loss: 0.0466 - val_accuracy: 0.9840 - val_loss: 0.0599\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9838 - loss: 0.0599\n",
      "Test Accuracy: 0.9837\n"
     ]
    }
   ],
   "source": [
    "# Build ANN Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=64, input_length=max_len),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(num_tags * max_len, activation=\"softmax\"),  # Output all tags at once\n",
    "    tf.keras.layers.Reshape((max_len, num_tags))  # Reshape output to sequence format\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 55ms/step - accuracy: 0.9512 - loss: 0.2830 - val_accuracy: 0.9888 - val_loss: 0.0471\n",
      "Epoch 2/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 57ms/step - accuracy: 0.9909 - loss: 0.0368 - val_accuracy: 0.9913 - val_loss: 0.0306\n",
      "Epoch 3/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 55ms/step - accuracy: 0.9933 - loss: 0.0231 - val_accuracy: 0.9914 - val_loss: 0.0286\n",
      "Epoch 4/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 55ms/step - accuracy: 0.9942 - loss: 0.0183 - val_accuracy: 0.9917 - val_loss: 0.0279\n",
      "Epoch 5/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 55ms/step - accuracy: 0.9948 - loss: 0.0158 - val_accuracy: 0.9917 - val_loss: 0.0288\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9917 - loss: 0.0285\n",
      "Test Accuracy: 0.9919\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=64, input_length=max_len),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),  # Single-directional LSTM\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_tags, activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chandan\\Desktop\\AgenticAI\\PS\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define BiLSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=64, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_tags, activation=\"softmax\"))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 63ms/step - accuracy: 0.9512 - loss: 0.2281 - val_accuracy: 0.9906 - val_loss: 0.0336\n",
      "Epoch 2/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 68ms/step - accuracy: 0.9927 - loss: 0.0259 - val_accuracy: 0.9924 - val_loss: 0.0256\n",
      "Epoch 3/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 99ms/step - accuracy: 0.9948 - loss: 0.0175 - val_accuracy: 0.9927 - val_loss: 0.0243\n",
      "Epoch 4/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 120ms/step - accuracy: 0.9958 - loss: 0.0137 - val_accuracy: 0.9931 - val_loss: 0.0241\n",
      "Epoch 5/5\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 83ms/step - accuracy: 0.9965 - loss: 0.0111 - val_accuracy: 0.9927 - val_loss: 0.0257\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9931 - loss: 0.0245\n",
      "Test Accuracy: 0.9931\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"ner_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Save word tokenizer\n",
    "with open(\"word_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word_tokenizer, f)\n",
    "\n",
    "# Save tag tokenizer\n",
    "with open(\"tag_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tag_tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug: * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chandan\\Desktop\\AgenticAI\\PS\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3557: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"ner_model.h5\")\n",
    "\n",
    "# Load tokenizers\n",
    "with open(\"word_tokenizer.pkl\", \"rb\") as f:\n",
    "    word_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(\"tag_tokenizer.pkl\", \"rb\") as f:\n",
    "    tag_tokenizer = pickle.load(f)\n",
    "\n",
    "# Get tag index mapping\n",
    "index_to_tag = {index: tag for tag, index in tag_tokenizer.word_index.items()}\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define prediction function\n",
    "def predict_ner(sentence):\n",
    "    words = sentence.split()\n",
    "    sequence = word_tokenizer.texts_to_sequences([words])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=model.input_shape[1], padding=\"post\")\n",
    "\n",
    "    predictions = model.predict(padded_sequence)\n",
    "    predicted_tags = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    output = [{\"word\": word, \"tag\": index_to_tag.get(tag, \"O\")} for word, tag in zip(words, predicted_tags[0])]\n",
    "    return output\n",
    "\n",
    "# Define API endpoint\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    sentence = data.get(\"sentence\", \"\")\n",
    "\n",
    "    if not sentence:\n",
    "        return jsonify({\"error\": \"No sentence provided\"}), 400\n",
    "\n",
    "    ner_result = predict_ner(sentence)\n",
    "    return jsonify({\"predictions\": ner_result})\n",
    "\n",
    "# Run Flask app\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Michael Jackson visited New York'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:5000/predict\"\n",
    "data = {\"sentence\": \"Elon Musk is the CEO of Tesla.\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
